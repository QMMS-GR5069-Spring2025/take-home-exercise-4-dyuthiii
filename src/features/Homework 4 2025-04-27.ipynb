{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0ddd60-ca01-4e0d-b1ab-dea79a1c5ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%python\n",
    "#%pip install mlflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65af570b-71ca-45ff-bfcf-45558eba8caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import avg, current_date, col, year, date_diff,floor, count, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "import mlflow.spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190f9b12-075e-4f08-83d3-fb5026326768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading in data from AWs\n",
    "drivers_df = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', \n",
    "                            header=True)\n",
    "display(drivers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75670238-f169-42b9-8dc8-4dcf0bd039eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_standings_df = spark.read.csv('s3://columbia-gr5069-main/raw/driver_standings.csv', \n",
    "                            header=True)\n",
    "display(driver_standings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694e8a41-e87c-4704-9e69-01b9d6da043f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pitstops_df = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', \n",
    "                            header=True)\n",
    "display(pitstops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ae6214-d9c5-47a2-b26a-881600658d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df = driver_standings_df.join(drivers_df, on='driverId')\n",
    "merged_df = merged_df.join(pitstops_df, on=['raceId','driverId'])\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa32e13-36fe-4a15-a4c5-b760234699e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating an age column\n",
    "#calclating AGE for each driver and adding it to the merged df\n",
    "merged_df = merged_df.withColumn('age', floor(date_diff(current_date(), 'dob') / 365))\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d721eec-8b64-4bdb-896a-7533e8792ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Running a regression model using random forest to predict position based on driverId, age, wins. number of pitstop stops, and pitstop duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c9d43b-fc31-4884-9a52-b56848961548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#converting to float and then merging\n",
    "merged_rfVars_df = merged_df.select('raceId','driverId','age','wins', 'position', 'stop', 'duration').selectExpr('cast(age as int) as age', 'cast(wins as int) as wins', 'cast(position as int) as position', 'cast(stop as int) as stop', 'cast(duration as float) as duration', 'cast(raceId as int) as raceId', 'cast(driverId as int) as driverId').dropna(how=\"any\")\n",
    "display(merged_rfVars_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5fffe69-5988-499c-ae9c-7c64951928bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "grouping by raceID before train-test split\n",
    "\n",
    "I will be using driverId, pitstop stops, duration and age to predict the number of wins a driver has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874c60f6-ea5e-4aac-8f3d-3b5e31cc50da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = ['driverId','age', 'raceId', 'duration','stop']\n",
    "target = 'wins'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_rfVars_df.select('driverId','age', 'raceId', 'duration','stop').toPandas(), merged_rfVars_df[['wins']].toPandas().values.ravel(), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1079fd98-763e-4f74-8388-b4bccf0c0222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26286c48-ddd3-4fca-b786-da81c0180f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name='RandomForest') as run:\n",
    "  # Create model, train it, and create predictions\n",
    "  rf = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "  rfFit= rf.fit(X_train, y_train)\n",
    "  predictionsRF = rf.predict(X_test)\n",
    "\n",
    "  # Log model\n",
    "  mlflow.sklearn.log_model(rf, \"RandomForestRegressor-model\")\n",
    "\n",
    "  # Create metrics\n",
    "  rmse_rf = mean_squared_error(y_test, predictionsRF, squared=False)\n",
    "  mse_rf = mean_squared_error(y_test, predictionsRF)\n",
    "  mae_rf = mean_absolute_error(y_test, predictionsRF)\n",
    "  r2_rf = r2_score(y_test, predictionsRF)\n",
    "\n",
    "  \n",
    "  # Log model and metrics\n",
    "  mlflow.sklearn.log_model(rfFit, \"random_forest_model\")\n",
    "  mlflow.log_metric(\"rmse\", rmse_rf)\n",
    "  mlflow.log_metric(\"r2\", r2_rf)\n",
    "  mlflow.log_metric(\"mae\", mae_rf)\n",
    "  mlflow.log_metric(\"mse\", mse_rf)\n",
    "  mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "  mlflow.log_param(\"numTrees\", 100)\n",
    "  mlflow.log_param(\"maxDepth\", 10)\n",
    "  # Saving and logging prediction CSV\n",
    "  predRF_final = pd.DataFrame({\n",
    "    'features': X_test.values.tolist(),\n",
    "    'target': y_test,\n",
    "    'prediction': predictionsRF\n",
    "  })\n",
    "  rf_csv_path = \"/tmp/rf_predictions.csv\"\n",
    "  predRF_final.to_csv(rf_csv_path, index=False)\n",
    "  mlflow.log_artifact(rf_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622b4eeb-1304-4416-85b8-f83f0a36d6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now the other model I will try is Linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df26247-4df0-455c-af2b-767f095a23ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using a linear regression with ML Flow to predict the target 'positionOrder'.\n",
    "mlflow.end_run() \n",
    "with mlflow.start_run(run_name=\"LinearRegression\"):\n",
    "    lr = LinearRegression()\n",
    "    lrFit= lr.fit(X_train, y_train)\n",
    "    predictionsLR = lr.predict(X_test)\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(lr, \"LinearRegression-model\")\n",
    "\n",
    "    # Create metrics\n",
    "    rmse_lr = mean_squared_error(y_test, predictionsLR, squared=False)\n",
    "    mse_lr = mean_squared_error(y_test, predictionsLR)\n",
    "    mae_lr = mean_absolute_error(y_test, predictionsLR)\n",
    "    r2_lr = r2_score(y_test, predictionsLR)\n",
    "\n",
    "    # Log model and metrics\n",
    "    mlflow.sklearn.log_model(lrFit, \"linear_regression_model\")\n",
    "    mlflow.log_metric(\"rmse\", rmse_lr)\n",
    "    mlflow.log_metric(\"r2\", r2_lr)\n",
    "    mlflow.log_metric(\"mae\", mae_lr)\n",
    "    mlflow.log_metric(\"mse\", mse_lr)\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "\n",
    "    #Saving and logging prediction CSV as a second artifact\n",
    "    predLR_final = pd.DataFrame({\n",
    "        'features': X_test.values.tolist(),\n",
    "        'target': y_test,\n",
    "        'prediction': predictionsLR\n",
    "    })\n",
    "    lr_csv_path = \"/tmp/lr_predictions.csv\"\n",
    "    predLR_final.to_csv(lr_csv_path, index=False)\n",
    "    mlflow.log_artifact(lr_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922d8b2e-81c7-426b-9c4b-6d69d44af144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Spark DataFrames\n",
    "predRF_final_spark = spark.createDataFrame(predRF_final)\n",
    "predLR_final_spark = spark.createDataFrame(predLR_final)\n",
    "\n",
    "# Convert array columns to strings (assuming 'features' is the array column)\n",
    "predRF_final_spark = predRF_final_spark.withColumn(\n",
    "    'features', concat_ws(',', col('features'))\n",
    ")\n",
    "predLR_final_spark = predLR_final_spark.withColumn(\n",
    "    'features', concat_ws(',', col('features'))\n",
    ")\n",
    "\n",
    "# Saving predictions to tables\n",
    "predRF_final_spark.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://dnd2129-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='rf_model_predictions',\n",
    "    user='admin',\n",
    "    password='dyuthi321'\n",
    ").mode('overwrite').save()\n",
    "\n",
    "\n",
    "predLR_final_spark.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://dnd2129-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='lr_model_predictions',\n",
    "    user='admin',\n",
    "    password='dyuthi321'\n",
    ").mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea68f530-dbbe-40e7-b6fd-2370d46306c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Viewing Random Forest predictions\n",
    "spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://dnd2129-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"rf_model_predictions\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"dyuthi321\") \\\n",
    "    .load().display()\n",
    "\n",
    "# Viewing Linear Regression predictions\n",
    "spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://dnd2129-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"lr_model_predictions\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"dyuthi321\") \\\n",
    "    .load().display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Homework 4 2025-04-27",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
